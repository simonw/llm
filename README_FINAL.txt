================================================================================
  COST ESTIMATION FEATURE - FINAL IMPLEMENTATION PLAN
================================================================================

STATUS: READY TO IMPLEMENT âœ…

WHAT CHANGED IN FINAL REVISION
-------------------------------
âœ… Do NOT bundle pricing data with package
âœ… Fetch from llm-prices.com on first use
âœ… Cache in user_dir() / "historical-v1.json"
âœ… 24-hour cache refresh (not 7 days)
âœ… Both sync and async Response.cost() methods
âœ… Integrated with -u/--usage flag (not separate commands)


QUICK START
-----------
1. READ: FINAL_PLAN_SUMMARY.md (complete overview)
2. READ: CACHING_UPDATE.md (caching implementation details)
3. IMPLEMENT: Follow IMPLEMENTATION_CHECKLIST_FINAL.md


USER EXPERIENCE
---------------
$ llm "Hello world" -m gpt-4 -u
Hello! How can I help you today?

Token usage: 10 input, 5 output, Cost: $0.000450 ($0.000300 input, $0.000150 output)
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                       NEW: Automatic cost estimation!


WHAT GETS BUILT
---------------
New Files:
  âœ… llm/costs.py                  CostEstimator + AsyncCostEstimator
  âœ… tests/test_costs.py           Comprehensive test suite

Modified Files:
  âœ… llm/models.py                 Response.cost() + AsyncResponse.cost()
  âœ… llm/utils.py                  Enhanced token_usage_string()
  âœ… llm/cli.py                    Updated usage display (2 places)
  âœ… llm/__init__.py               Export new classes

Runtime (User Directory):
  âœ… ~/.local/share/io.datasette.llm/historical-v1.json  (cached pricing)


CACHING STRATEGY
----------------
First Use:
  â†’ Fetch from https://www.llm-prices.com/historical-v1.json
  â†’ Save to user_dir()/historical-v1.json
  â†’ Display cost

Subsequent Uses (< 24 hours):
  â†’ Load from cache (instant)
  â†’ Display cost

After 24 Hours:
  â†’ Try to fetch fresh data
  â†’ If success: update cache
  â†’ If failure: use stale cache
  â†’ Display cost either way

Network Unavailable:
  â†’ Use stale cache if available
  â†’ Otherwise: skip cost silently (no error)


SYNC vs ASYNC
-------------
Response.cost()              â†’ Synchronous (may block on first fetch)
AsyncResponse.cost()         â†’ Asynchronous (proper async/await)

Both use the same caching mechanism, just different I/O patterns.


ARCHITECTURE SUMMARY
--------------------
User runs: llm "prompt" -m gpt-4 -u
    â†“
token_usage_string() called
    â†“
get_default_estimator() (singleton)
    â†“
First use? â†’ Fetch + Cache
Cached?    â†’ Load cache
    â†“
calculate_cost()
    â†“
Display: "Token usage: X input, Y output, Cost: $Z"


KEY FILES
---------
Essential Documentation:
  â€¢ FINAL_PLAN_SUMMARY.md               Complete overview
  â€¢ CACHING_UPDATE.md                   Detailed caching implementation
  â€¢ IMPLEMENTATION_CHECKLIST_FINAL.md   Step-by-step tasks
  â€¢ COMPARISON.md                       Why this approach

Reference:
  â€¢ COST_ESTIMATION_PLAN.md             Original technical plan
  â€¢ COST_ARCHITECTURE.md                Architecture diagrams
  â€¢ EXAMPLE_TESTS.py                    Test examples

Superseded (Do Not Use):
  â€¢ IMPLEMENTATION_CHECKLIST_REVISED.md (use FINAL instead)
  â€¢ IMPLEMENTATION_CHECKLIST.md         (use FINAL instead)


IMPLEMENTATION PHASES
---------------------
Phase 1: Core (2 days)
  â€¢ Create llm/costs.py
  â€¢ CostEstimator (sync)
  â€¢ AsyncCostEstimator (async)
  â€¢ Cache management (24h TTL)
  â€¢ Cost calculation
  â€¢ Unit tests

Phase 2: Integration (0.5 day)
  â€¢ Response.cost() methods
  â€¢ Update exports
  â€¢ Integration tests

Phase 3: CLI (0.5 day)
  â€¢ Enhance token_usage_string()
  â€¢ Update CLI calls (2 places)
  â€¢ CLI tests

Phase 4: Polish (1 day)
  â€¢ Documentation
  â€¢ Error handling
  â€¢ Performance testing

TOTAL: 4 days


TESTING STRATEGY
----------------
Unit Tests:
  â€¢ Fetch/cache/refresh logic
  â€¢ Model ID matching
  â€¢ Cost calculations
  â€¢ Error handling

Integration Tests:
  â€¢ Response.cost() sync
  â€¢ AsyncResponse.cost() async
  â€¢ CLI integration
  â€¢ Network scenarios

Manual Testing:
  # First use
  rm ~/.local/share/io.datasette.llm/historical-v1.json
  llm "Test" -m gpt-4 -u
  
  # Cached use
  llm "Test" -m gpt-4 -u
  
  # Stale cache
  touch -t 202401010000 ~/.local/share/io.datasette.llm/historical-v1.json
  llm "Test" -m gpt-4 -u


ERROR HANDLING
--------------
âœ… Network unavailable â†’ Use stale cache or skip cost
âœ… Cache missing + network error â†’ Skip cost silently
âœ… Unknown model â†’ Skip cost, show tokens only
âœ… Invalid pricing data â†’ Log warning, skip cost
âœ… Never break existing functionality


SUCCESS CRITERIA
----------------
[x] Planning complete
[x] Caching strategy defined
[x] Sync/async approach decided
[ ] Pricing fetches on first use
[ ] Cache saves to correct location
[ ] 24-hour refresh works
[ ] Cost appears with -u flag
[ ] Both sync and async work
[ ] Tests >90% coverage
[ ] No breaking changes


KEY METRICS
-----------
Code to write:     ~600 lines (400 core + 200 integration)
Tests to write:    ~500 lines
Files modified:    4 (models.py, utils.py, cli.py, __init__.py)
Files created:     2 (costs.py, test_costs.py)
Implementation:    4 days estimated
Performance:       First use ~200-500ms, subsequent ~12ms
Cache size:        ~17KB (historical-v1.json)


NEXT STEPS
----------
1. Review FINAL_PLAN_SUMMARY.md
2. Review CACHING_UPDATE.md
3. Open IMPLEMENTATION_CHECKLIST_FINAL.md
4. Start Phase 1: Create llm/costs.py


RESOURCES
---------
Pricing data: https://www.llm-prices.com/historical-v1.json
LLM project:  https://github.com/simonw/llm
LLM docs:     https://llm.datasette.io/


================================================================================
                      READY TO START IMPLEMENTATION! ðŸš€
================================================================================
